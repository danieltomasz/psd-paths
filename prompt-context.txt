<documents>
<document index="1">
<source>./pyproject.toml</source>
<document_content>
[tool.ruff]
exclude = [
    ".bzr",
    ".direnv",
    ".eggs",
    ".git",
    ".git-rewrite",
    ".hg",
    ".ipynb_checkpoints",
    ".mypy_cache",
    ".nox",
    ".pants.d",
    ".pyenv",
    ".pytest_cache",
    ".pytype",
    ".ruff_cache",
    ".svn",
    ".tox",
    ".venv",
    ".vscode",
    "__pypackages__",
    "_build",
    "buck-out",
    "build",
    "dist",
    "node_modules",
    "site-packages",
    "venv",
]

# Same as Black.
line-length = 88
indent-width = 4

[tool.ruff.lint]
ignore = ["E302","E265"]

[tool.ruff.format]
ignore = ["E302","E265"]

</document_content>
</document>
<document index="2">
<source>./settings.toml</source>
<document_content>
[paths]
project_root = "/Users/daniel/PhD/Projects/psd-paths"

[experiment]
# Add any experiment-specific settings here
task = "rest" # or whatever your task is

[subjects]
# List of valid subject IDs if you want to validate
valid_ids = ["001", "002", "003"]

[preprocessing]
channels_to_remove = [
    "E67",
    "E73",
    "E82",
    "E91",
    "E92",
    "E102",
    "E111",
    "E120",
    "E133",
    "E145",
    "E165",
    "E174",
    "E187",
    "E199",
    "E208",
    "E209",
    "E216",
    "E217",
    "E218",
    "E219",
    "E225",
    "E226",
    "E227",
    "E228",
    "E229",
    "E230",
    "E231",
    "E232",
    "E233",
    "E234",
    "E235",
    "E236",
    "E237",
    "E238",
    "E239",
    "E240",
    "E241",
    "E242",
    "E243",
    "E244",
    "E245",
    "E246",
    "E247",
    "E248",
    "E249",
    "E250",
    "E251",
    "E252",
    "E253",
    "E254",
    "E255",
    "E256",
]

</document_content>
</document>
<document index="3">
<source>./tasks.py</source>
<document_content>
"""
File to run the pipeline.
Author: Daniel Borek, July 2023
"""

from pathlib import Path
import tomllib
import re
import invoke


VARS = "PYDEVD_DISABLE_FILE_VALIDATION=1"
VENV = "conda-paths-3.12"
ENV = "miniforge3-latest"

PYTHON = f"~/.pyenv/versions/{ENV}/envs/{VENV}/bin/python"

with open(".env.toml", "rb") as toml_file:
    config = tomllib.load(toml_file)
project_path = config["project"]["path"]


# template = "notebooks/test/test0.ipynb"
# output_dir = "notebooks/test"


def extract_numbers(bids_paths):
    # Initialize an empty list to store the numbers
    numbers = []

    # Get all the child directories
    for path in bids_paths.glob("*"):
        if path.is_dir():
            # Convert the path to a string
            str_path = str(path)

            # Find all three-digit numbers in the string
            matches = re.findall(r"\b\d{3}\b", str_path)

            # Add the numbers to the list
            numbers.extend(matches)

    # Now 'numbers' is a list of all three-digit numbers in the child directory names
    return sorted(numbers)


def run_parametrised_report(c, subject, output_dir, report, output_file):
    """Basic function to run a parametrised report."""
    try:
        if not (Path(output_dir) / output_file).exists():
            Path(output_dir).mkdir(parents=True, exist_ok=True)
            c.run(
                f"quarto render {report} \\"
                f"-P subject:{subject}  --to ipynb  \\"
                f"--output {output_file} "
            )
    except Exception as e:
        print(f"Error processing sub-{subject}: {e}")


@invoke.task(default=True)
def run_quarto_params(c):
    """
    Run parametrised quarto reports.
    """
    # get subjects
    bids_folder = Path(f"{project_path}/data/raw/BIDS/")
    subjects = extract_numbers(bids_folder)

    # set templates and output files
    output_dir = "analysis"

    templates = ["template-1-raw-notch-v2.ipynb", "template-2-ica-specparam-v2.ipynb"]

    # iterate over subjects
    for subject in subjects:
        output_files = [
            f"sub-{subject}_1-raw-notch-v2.ipynb",
            f"sub-{subject}_2-ica-specparam-v2.ipynb",
        ]
        for template, output_file in zip(templates, output_files):
            print(f"Processing sub-{subject}")
            report = f"notebooks/1-preprocessing/{template}"
            run_parametrised_report(c, subject, output_dir, report, output_file)


@invoke.task()
def run_specparam(c):
    """
    Run parametrised quarto reports.
    """
    # get subjects
    bids_folder = Path(f"{project_path}/data/raw/BIDS/")
    subjects = extract_numbers(bids_folder)

    # set templates and output files
    output_dir = "analysis/last_stage"

    template = "template-4-specparam.ipynb"

    # iterate over subjects
    for subject in subjects:
        output_file = f"sub-{subject}_4-specparam.ipynb"
        print(f"Processing sub-{subject}")
        report = f"notebooks/1-preprocessing/{template}"
        run_parametrised_report(c, subject, output_dir, report, output_file)


@invoke.task()
def pipeline_notch(c):
    """
    Run parametrised quarto reports.
    """
    # get subjects
    bids_folder = Path(f"{project_path}/data/raw/BIDS/")
    subjects = extract_numbers(bids_folder)

    # set templates and output files
    output_dir = "analysis"

    template = "template-1-raw-notch-v2.ipynb"

    # iterate over subjects
    for subject in subjects:
        output_file = f"sub-{subject}_1-raw-notch-v2.ipynb"
        print(f"Processing sub-{subject}")
        report = f"notebooks/1-preprocessing/{template}"
        run_parametrised_report(c, subject, output_dir, report, output_file)


@invoke.task()
def final_ica(c):
    """
    Run parametrised quarto reports.
    """
    # get subjects
    bids_folder = Path(f"{project_path}/data/raw/BIDS/")
    subjects = extract_numbers(bids_folder)

    # set templates and output files
    output_dir = "analysis"

    template = "template-3-FinalisingICA.ipynb"

    # iterate over subjects
    for subject in subjects:
        output_file = f"sub-{subject}_3-FinalisingICA.ipynb"
        print(f"Processing sub-{subject}")
        report = f"notebooks/1-preprocessing/{template}"
        run_parametrised_report(c, subject, output_dir, report, output_file)


@invoke.task()
def final_specparam(c):
    """
    Run parametrised quarto reports.
    """
    # get subjects
    bids_folder = Path(f"{project_path}/data/raw/BIDS/")
    subjects = extract_numbers(bids_folder)

    # set templates and output files
    output_dir = "analysis"
    template = "template-4-specparam-v2.ipynb"

    # iterate over subjects
    for subject in subjects:
        output_file = f"sub-{subject}_4-specparam.ipynb"
        print(f"Processing sub-{subject}")
        report = f"notebooks/1-preprocessing/{template}"
        run_parametrised_report(c, subject, output_dir, report, output_file)

</document_content>
</document>
<document index="4">
<source>./scripts/1-preprocessing/1-convert2BIDS.py</source>
<document_content>

"""
This script converts EEG data in MFF format to BIDS format.

Author: Daniel Borek
Date: 2024-03-12
"""
# %%
from pathlib import Path
import re
import mne
import toml
import mne_bids

config = toml.load("../../.env.toml")

project_path = config["project"]["path"]
dir_path = config["project"]["raw_dir_path"] #Where the data is stored

output_path = f"{project_path}/data/raw/BIDS" #Copy the data to this location
Path(output_path).mkdir(parents=True, exist_ok=True)

def convert2bids(path, sub, task='rest', test=True):
    """
    Use MNE-BIDS to convert the EEG data to BIDS format.
    """
    raw = mne.io.read_raw_egi(path, preload=True )
    raw.set_montage('GSN-HydroCel-257', match_alias={'VREF' :'Cz'})
    #raw.plot_sensors(show_names=True);
    bids_path = mne_bids.BIDSPath(subject=sub,
                                session='01',
                                task= task,
                                datatype='eeg',
                                root=output_path )
    mne_bids.write_raw_bids(raw, bids_path=bids_path,  format='EEGLAB',allow_preload=True,
                            overwrite=True);

# %% Run the conversion in a loop
# List all files in the directory
files = [file for file in Path(dir_path).glob('*.mff')]

#%%
for fname in files:
    # Use regex to find the first number after "PATHS"
    match = re.search(r'(?<=PATHS_)(\d+)|(?<=PATHS_)(\d+)', fname.stem)
    if match:
        sub = match.group(1)
    convert2bids(fname, sub)
    print(f"Converted {fname} to BIDS format")

# %%

replacements = {
    "N006": "132",
    "N013": "135",
    "N014": "136"
}

for fname in files:
    for old_name, sub in replacements.items():
        if old_name in fname.name:
            print(old_name)
            convert2bids(fname, sub)
            print(f"Converted {fname} to BIDS format")
# %%

</document_content>
</document>
<document index="5">
<source>./src/spectral/pyproject.toml</source>
<document_content>
[build-system]
requires = ["setuptools>=61",  "setuptools_scm[toml]>=6.2"]
build-backend = "setuptools.build_meta"

[project]
name = "spectral"
authors = [{ name = "Daniel Borek", email = "daniel.borek@ugent.be" }]
readme = "README.md"
version = "0.0.1.dev0"
description = "Project for working with MEG  and EEG data"
license = { file = "LICENSE" }
classifiers = ["License :: OSI Approved :: MIT License"]

requires-python = ">=3.10"

dependencies = [
    "mne",
    "specparam>=2.0.0rc1",
    "numpy",
    "pandas",]


[tool.setuptools.packages.find]
include = ["spectral*"]
namespaces = false

[tool.pytest.ini_options]
pythonpath = [
    "."
]
addopts = "-v"
testpaths = ["docs"]

[tool.black]
line-length = 88
target-version = ['py310', 'py311']
include = '\.pyi?$'

# iSort
[tool.isort]
profile = "black"
line_length = 88
multi_line_output = 3
include_trailing_comma = true


[tool.flake8]
ignore = ['E231', 'E241']
per-file-ignores = [
    '__init__.py:F401',
]
max-line-length = 88
count = true


[tool.pytest-watcher]
now = false
delay = 1.0
runner = "pytest"
runner_args = ["--picked", "--testmon", "--ignore-glob='test_data_preproc.py'"]
patterns = ["*.py"]
ignore_patterns = []

[tool.ruff]
exclude = [
    ".bzr",
    ".direnv",
    ".eggs",
    ".git",
    ".git-rewrite",
    ".hg",
    ".ipynb_checkpoints",
    ".mypy_cache",
    ".nox",
    ".pants.d",
    ".pyenv",
    ".pytest_cache",
    ".pytype",
    ".ruff_cache",
    ".svn",
    ".tox",
    ".venv",
    ".vscode",
    "__pypackages__",
    "_build",
    "buck-out",
    "build",
    "dist",
    "node_modules",
    "site-packages",
    "venv",
]

# Same as Black.
line-length = 88
indent-width = 4

[tool.ruff.lint]
ignore = ["E302","E265"]

[tool.ruff.format]
ignore = ["E302","E265"]

</document_content>
</document>
<document index="6">
<source>./src/spectral/spectral/__init__.py</source>
<document_content>
from .utils import print_timestamp # noqa: F401
</document_content>
</document>
<document index="7">
<source>./src/spectral/spectral/epochs.py</source>
<document_content>
import mne
import autoreject
from mne import Epochs


def create_epochs(raw: mne.io.Raw, length: float = 5, overlap: float = 1.5):
    """Create epochs from raw data"""
    events = mne.make_fixed_length_events(
        raw=raw,
        start=0,
        stop=None,
        duration=length,
        overlap=overlap,
        first_samp=False,
    )
    epochs = Epochs(
        raw=raw,
        events=events,
        tmin=0,
        tmax=length,
        detrend=1,  # from 0 to 1
        baseline=None,
        preload=True,
        reject_by_annotation=True,
    )
    return epochs


def get_reject_log(epochs, resample=None, consensus=[0.8], n_interpolate=None):
    """Get reject log from epochs"""
    if n_interpolate is None:
        n_interpolate = [1, 2, 16, 32, 64, 128]
    if resample:
        eeg_epochs = epochs.copy().resample(resample)
    else:
        eeg_epochs = epochs.copy()
    auto_reject_pre_ica = autoreject.AutoReject(
        n_interpolate=n_interpolate,
        n_jobs=-1,
        random_state=100,
        thresh_method="bayesian_optimization",
        verbose=False,
        # n_interpolate=np.array([0]),
        consensus=consensus,
    ).fit(eeg_epochs)
    print("fitting finished")
    _, reject_log = auto_reject_pre_ica.transform(eeg_epochs, return_log=True)
    # print(reject_log.bad_epochs)
    return reject_log

</document_content>
</document>
<document index="8">
<source>./src/spectral/spectral/ica.py</source>
<document_content>
from mne_icalabel import label_components
from mne.preprocessing import ICA


def compute_ica(
    eeg_data,
    reject_log=None,
    n_components=0.999,
    method="picard",
    random_state: int = 99,
):
    """Compute ICA on the data without really bad epochs"""
    ica = ICA(
        n_components=n_components,
        random_state=random_state,
        method="picard",
        fit_params=dict(ortho=False, extended=True),
    )
    ica.fit(eeg_data)

    return ica


def label_components_ica(eeg_data, ica):
    """Assign the IC labels"""

    ic_labels = label_components(eeg_data, ica, method="iclabel")
    labels = ic_labels["labels"]
    return labels, ic_labels


def get_values(labels, ic_labels, threshold=0.8):
    """Get the indices of the labels"""
    element_indices = {}
    element_indices["bad_prob_class"] = []
    prababilities = ic_labels["y_pred_proba"]

    for i, element in enumerate(labels):
        if prababilities[i] > 0.8:
            if element not in element_indices:
                element_indices[element] = []
            element_indices[element].append(i)
        else:
            element_indices["bad_prob_class"].append(i)

    # display(element_indices)
    return element_indices


def plot_ica_components(ica, eeg_data, subject, figures_path, plot_properties=[]):
    """Plot the ICA components"""

    labels, ic_labels = label_components_ica(eeg_data, ica)
    labeled_components = get_values(labels, ic_labels)

    for label, indices in labeled_components.items():
        # display(label, indices)
        # ica_plot = ica.plot_sources(
        #    eeg_data, show_scrollbars=False, picks=slice(0, 20))
        print(label, indices)
        ica_plot_components = ica.plot_components(picks=indices)

        chunk_size = 20
        chunks = [
            indices[i : i + chunk_size] for i in range(0, len(indices), chunk_size)
        ]
        for chunk in chunks:
            ica_plot = ica.plot_sources(
                eeg_data,
                show_scrollbars=False,
                picks=chunk,
                start=0,
                stop=len(eeg_data) - 1,
            )

        if plot_properties:
            ica.plot_properties(eeg_data, picks=indices, psd_args={"fmax": 100.0})
    return labeled_components


def plot_removed_components(
    ica,
    eeg_data,
    figure_path,
    subject,
    labeled_components,
    chosen_components=[
        "eye blink",
        "heart beat",
        "line noise",
        "muscle artifact",
        "channel noise",
    ],
):
    """Plot the components labeled as eye blink, heart beat, line noise, muscle artifact, channel noise"""
    exclude = []
    for label, indices in labeled_components.items():
        if label in chosen_components:
            ica_plot = ica.plot_sources(eeg_data, show_scrollbars=False, picks=indices)
            print(label, indices)
            figs = ica.plot_properties(
                eeg_data, picks=indices, psd_args={"fmax": 100.0}
            )

            for fig, ind in zip(figs, indices):
                # fig = ica.plot_properties(eeg_data, picks=ind)
                fig.savefig(
                    f"{figure_path}/sub-{subject}_ICA{str(ind).zfill(3)}_{
                        label.replace(" ", "-")}.png",
                    dpi=300,
                    bbox_inches="tight",
                )
            exclude = exclude + indices
    return exclude

</document_content>
</document>
<document index="9">
<source>./src/spectral/spectral/paths_utils.py</source>
<document_content>
import os
import copy
import pandas as pd
import numpy as np
import mne
import matplotlib.pyplot as plt

# from mne.preprocessing import ICA
# from mne_icalabel.gui import label_ica_components
from mne_icalabel import label_components

import autoreject
from specparam.plts.spectra import plot_spectra
from specparam import SpectralGroupModel
import matplotlib.pyplot as plt
import matplotlib.colors as colors
import numpy as np
import re
import pandas as pd
import numpy as np
from specparam.core.funcs import infer_ap_func
from specparam.core.info import get_ap_indices
import datetime


def compare_before_after(
    epochs_before,
    epochs_after,
    subject: int,
    fmax: float = 40.0,
    title: str | None = None,
):
    """Compare PSD of the data befre and after cleaning"""
    if title is None:
        title = f"ICA Comparison for subject {subject}"
    fig_psd, axs = plt.subplots(2, 1, sharex=True, sharey=True, figsize=(10, 8))

    epochs_before.compute_psd(
        method="welch", picks="eeg", fmax=fmax, exclude="bads"
    ).plot(
        average=False,
        picks="eeg",
        exclude="bads",
        show=False,
        amplitude=False,
        axes=axs[0],
    )
    axs[0].set_title(f"Before  for subject {subject}")

    epochs_after.compute_psd(
        method="welch", picks="eeg", fmax=fmax, exclude="bads"
    ).plot(
        average=False,
        picks="eeg",
        exclude="bads",
        show=False,
        amplitude=False,
        axes=axs[1],
    )
    axs[1].set_title(f"After for subject {subject}")
    fig_psd.suptitle(title, fontsize=16)

    return fig_psd


def create_epochs(
    raw: mne.io.Raw,
    epochs_params: dict | None = None,
    length: float = 5,
    overlap: float = 1.5,
):
    """Create epochs from raw data"""
    # Create fixed length events
    events = mne.make_fixed_length_events(
        raw=raw, start=0, stop=None, duration=length, overlap=overlap, first_samp=True
    )

    # Default parameters for epochs
    default_params = {
        "raw": raw,
        "events": events,
        "tmin": 0,
        "tmax": length,
        "detrend": 1,  # Linear detrending
        "baseline": None,
        "preload": True,
        "reject_by_annotation": None,
        "reject": None,
    }
    # Update default parameters with the ones passed to the function
    if epochs_params is not None:
        default_params.update(epochs_params)

    return mne.Epochs(**default_params)


def switch_bad_to_interpolate(reject_log):
    """
    Switch all bad epochs to epochs to be interpolated in a RejectLog instance.

    Parameters
    ----------
    reject_log : RejectLog
        The RejectLog instance to modify.

    Returns
    -------
    RejectLog
        A new RejectLog instance with the modifications applied.
    """

    # Create a copy of the original labels
    new_labels = reject_log.labels.copy()

    # Find all bad epochs
    bad_epoch_indices = np.where(reject_log.bad_epochs)[0]

    # Print the indexes of bad epochs
    print("Indexes of bad epochs:")
    print(bad_epoch_indices)  # epochs doesnt start from 0

    # For each bad epoch, set all channels to be interpolated (value 2)
    for epoch_idx in bad_epoch_indices:
        new_labels[epoch_idx, :] = 2

    # Create a new RejectLog instance with the modified labels
    new_reject_log = autoreject.RejectLog(
        bad_epochs=np.zeros_like(reject_log.bad_epochs, dtype=bool),
        labels=new_labels,
        ch_names=reject_log.ch_names,
    )

    return new_reject_log


def update_reject_log(orginal_reject_log, bad_epochs_indices, new_label=1):
    """
    Update bad epochs and labels in the RejectLog object.

    Parameters
    ----------
    reject_log : RejectLog
        An instance of the RejectLog class.
    bad_epochs_indices : list of int
        List of epoch indices to be marked as bad.
    new_label : int, optional
        The label to set for the bad epochs in the labels array.
        Default is 1 (bad).

    Returns
    -------
    New instance (copy) of reject_log
    """
    reject_log = copy.deepcopy(orginal_reject_log)
    for idx in bad_epochs_indices:
        if idx < len(reject_log.bad_epochs):
            reject_log.bad_epochs[idx] = True
            reject_log.labels[idx, :] = new_label
        else:
            raise ValueError(
                f"Index {idx} is out of range for bad_epochs of length {len(reject_log.bad_epochs)}"
            )
    return reject_log


def exclude_bad_channels(epochs):
    """Exclude bad channels from the montage (for plotting only good channels)"""
    all_channels = epochs.info["ch_names"]
    bad_channels = epochs.info["bads"]
    good_channels = [ch for ch in all_channels if ch not in bad_channels]
    epochs_good = epochs.copy().pick_channels(good_channels)
    return epochs_good


def plot_specparam_on_scalp(fg, epochs, subject):
    """Plot aperiodc values and its goodness of fit across the scalp"""
    # Extract aperiodic exponent values
    exps = fg.get_params("aperiodic_params", "exponent")
    r_squared = fg.get_params("r_squared")

    # Assuming 'exps' is your data array and 'raw' is an MNE raw object
    # Also, assuming 'unit_label' and 'fontsize' variables are defined

    fig, axs = plt.subplots(1, 2, figsize=(10, 5))

    # The 'cmap' parameter expects a colormap object, not a string
    im1, _ = mne.viz.plot_topomap(
        exps, epochs.info, axes=axs[0], cmap="viridis", contours=0, show=False
    )
    axs[0].set_title("Exponent Values")

    #  Colorbar setup for the first subplot at the bottom
    cbar_ax1 = fig.add_axes((0.1, 0.05, 0.35, 0.03))
    fig.colorbar(im1, cax=cbar_ax1, orientation="horizontal")

    # Plot the 'errors' data in the second subplot
    im2, _ = mne.viz.plot_topomap(
        r_squared,
        epochs.info,
        axes=axs[1],
        cmap="plasma",
        contours=0,
        show=False,
    )
    axs[1].set_title("R_squared Values")

    # Colorbar setup for the second subplot at the bottom
    cbar_ax2 = fig.add_axes((0.55, 0.05, 0.35, 0.03))
    fig.colorbar(im2, cax=cbar_ax2, orientation="horizontal")
    fig.suptitle(f"sub-{subject} - Exponent and R_squared values")

    plt.show()


def specparam2pandas(fg):
    """
    Converts a SpectralGroupModel object into a pandas DataFrame, with peak parameters and
    corresponding aperiodic fit information.

    Args:
    -----
    fg : specpramGroup
        The SpectralGroupModel object containing the fitting results.

    Returns:
    --------
    peaks_df : pandas.DataFrame
        A DataFrame with the peak parameters and corresponding aperiodic fit information.
        The columns are:
        - 'CF': center frequency of each peak
        - 'PW': power of each peak
        - 'BW': bandwidth of each peak
        - 'error': fitting error of the aperiodic component
        - 'r_squared': R-squared value of the aperiodic fit
        - 'exponent': exponent of the aperiodic component
        - 'offset': offset of the aperiodic component
        - 'knee': knee parameter of the aperiodic component [if is initially present in the fg object]
    Notes:
    ------
    This function creates two DataFrames. The first DataFrame `specparam_aperiodic`
    contains the aperiodic fit information and is based on the `aperiodic_params`
    attribute of the SpectralGroupModel object. The columns are inferred using the
    `get_ap_indices()` and `infer_ap_func()` functions from the specparam package.
    The second DataFrame `peak_df` contains the peak parameters and is based on the
    `peak_params` attribute of the SpectralGroupModel object. The column names are renamed
    to match the headers of `fooof_aperiodic`, and the 'ID' column is cast to integer.
    The two DataFrames are then merged based on a shared 'ID' column.
    """

    specparam_aperiodic = (
        pd.DataFrame(
            fg.get_params("aperiodic_params"),
            columns=get_ap_indices(
                infer_ap_func(np.transpose(fg.get_params("aperiodic_params")))
            ),
        )
        .assign(error=fg.get_params("error"), r_squared=fg.get_params("r_squared"))
        .reset_index(names=["ID"])
    )
    return (
        pd.DataFrame(fg.get_params("peak_params"))  # prepare peaks dataframe
        .set_axis(["CF", "PW", "BW", "ID"], axis=1)  # rename cols
        .astype({"ID": int})
        .join(specparam_aperiodic.set_index("ID"), on="ID")
    )


def examine_spectra(fg, subject):
    """Compare the power spectra between low and high exponent channels"""
    fig, ax = plt.subplots(1, 2, figsize=(12, 6))

    def argmedian(arr):
        return np.argsort(arr)[len(arr) // 2]

    exps = fg.get_params("aperiodic_params", "exponent")
    r_squared = fg.get_params("r_squared")
    spectra_exp = [
        fg.get_model(np.argmin(exps)).power_spectrum,
        fg.get_model(argmedian(exps)).power_spectrum,
        fg.get_model(np.argmax(exps)).power_spectrum,
    ]

    labels_spectra_exp = [
        f"Low Exponent {format(np.min(exps), '.2f')}",
        f"Median Exponent {format(np.median(exps), '.2f')}",
        f"High Exponent {format(np.max(exps), '.2f')}",
    ]

    plot_spectra(
        fg.freqs,
        spectra_exp,
        ax=ax[0],
        labels=labels_spectra_exp,
    )

    spectra_r_squared = [
        fg.get_model(np.argmin(r_squared)).power_spectrum,
        fg.get_model(argmedian(r_squared)).power_spectrum,
        fg.get_model(np.argmax(r_squared)).power_spectrum,
    ]

    labels_spectra_r_squared = [
        f"Low R_squared  {format(np.min(r_squared), '.2f')}",
        f"Median R_squared {format(np.median(r_squared), '.2f')}",
        f"High R_squared {format(np.max(r_squared), '.2f')}",
    ]

    my_colors = ["blue", "green", "red"]
    plot_spectra(
        fg.freqs,
        spectra_r_squared,
        ax=ax[1],
        labels=labels_spectra_r_squared,
        colors=my_colors,
    )
    ylim1 = ax[0].get_ylim()
    ylim2 = ax[1].get_ylim()
    # Set the same limits on the y-axis for both plots
    ax[0].set_ylim(min(ylim1[0], ylim2[0]), max(ylim1[1], ylim2[1]))
    ax[1].set_ylim(min(ylim1[0], ylim2[0]), max(ylim1[1], ylim2[1]))
    fig.suptitle(
        f"sub-{subject} - Power spectra comparison between low, median and high exponent and R_squared values"
    )


def plot_spectra_models_generalized(fg, data, data_type="exps"):
    """
    Plots spectra models based on specified data type (experimental data or r_squared).

    Parameters:
    - fg: The FOOOFGroup object.
    - data: Array-like, data to determine models (experimental data or r_squared).
    - labels: Labels for each plotted spectra model.
    - data_type: Type of data to plot ('exps' for experimental data, 'r_squared' for r_squared values).
    """

    # Define a helper function for median
    def argmedian(data):
        return np.argsort(data)[len(data) // 2]

    # Select the appropriate data for model generation
    if data_type == "exps":
        indices = [np.argmin(data), argmedian(data), np.argmax(data)]
        labels = [
            f"Low {data_type} {format(np.min(data), '.2f')}",
            f"Median {data_type} {format(np.median(data), '.2f')}",
            f"High {data_type} {format(np.max(data), '.2f')}",
        ]
    elif data_type == "r_squared":
        indices = [np.argmin(data), argmedian(data), np.argmax(data)]
        labels = [
            f"Low R_squared {format(np.min(data), '.2f')}",
            f"Median R_squared {format(np.median(data), '.2f')}",
            f"High R_squared {format(np.max(data), '.2f')}",
        ]
    else:
        raise ValueError("data_type must be 'exps' or 'r_squared'")

    # Generate models based on the selected data
    spectra_models = [fg.get_model(idx, regenerate=True) for idx in indices]

    # Iterate over each model and its corresponding label
    for model, label in zip(spectra_models, labels):
        # Print results and plot extracted model fit
        model.print_results()
        model.plot()
        print(label)


def extract_elements(df, subject_id, column_name):
    """
    Extract numerical elements from a specified column for a given subject in a DataFrame.

    Args:
    df (pandas.DataFrame): The DataFrame containing the data.
    subject_id (int): The ID of the subject to extract data for.
    column_name (str): The name of the column to extract data from.

    Returns:
    list: A list of integers extracted from the specified column for the given subject.
        Returns an empty list if no numbers are found or if the cell is empty/NaN.

    Example:
    >>> df = pd.read_excel("data.xlsx")
    >>> elements = extract_elements(df, 104, "ICA_3Take")
    >>> print(elements)
    [1, 2, 3, 4]  # Example output
    """
    # Find the row for the subject
    row = df.loc[df["ID"] == subject_id, ["ID", column_name]]

    # Extract the string from the specified column
    elements_string = row[column_name].values[0]

    # Initialize an empty list for the elements
    elements = []

    # Extract numbers from the string if it's not NaN
    if pd.notna(elements_string):
        elements = [
            int(float(num))
            for num in re.findall(r"(?<!\d)-?\d+(?:\.\d+)?(?!\d)", str(elements_string))
        ]
    return elements


def save_specparam_results(
    fg, epochs_ar, ica, subject, n_interpolated_channels, path=None
):
    """Save the results of the spectral parameterization to a CSV file"""
    channel_names = epochs_ar.info["ch_names"]
    df_channels = pd.DataFrame({"ID": range(len(channel_names)), "ch": channel_names})

    df = specparam2pandas(fg)
    df = df.merge(df_channels, on="ID")
    df["sub_id"] = subject

    # Get the current date and time
    now = datetime.datetime.now()
    df["timestamp"] = now
    df["nr_intepolated_channels"] = n_interpolated_channels
    df["nr_dropped_ica"] = len(ica.exclude)
    df["nr_retained_epochs"] = len(epochs_ar)
    df["nr_retained_ica"] = ica.n_components_ - len(ica.exclude)
    # Create a new list of column names
    cols = ["ch"] + [col for col in df.columns if col != "ch"]

    # Reorder the columns
    df = df[cols]
    if path is None:
        path = f"specparam/sub-{subject}-specparam.csv"
    df.to_csv(path, index=False)
    print(f"Subject {subject} done")
    return df


def plot_models(fg, param_choice="exponent"):
    """
    Plot models from a FOOOF group object based on exponent or R-squared values.

    This function generates three plots (low, median, and high) for the specified
    parameter, prints the results for each model, and displays the corresponding label.

    Parameters:
    -----------
    fg : FOOOFGroup
        The FOOOF group object containing the models to plot.
    param_choice : str, optional
        The parameter to use for selecting models. Must be either 'exponent' or 'r_squared'.
        Default is 'exponent'.

    Raises:
    -------
    ValueError
        If param_choice is not 'exponent' or 'r_squared'.
    """
    if param_choice.lower() == "exponent":
        param = fg.get_params("aperiodic_params", "exponent")
        param_name = "Exponent"
    elif param_choice.lower() == "r_squared":
        param = fg.get_params("r_squared")
        param_name = "R-squared"
    else:
        raise ValueError("param_choice must be either 'exponent' or 'r_squared'")

    def argmedian(arr):
        return np.argsort(arr)[len(arr) // 2]

    labels_spectra = [
        f"Low {param_name} {format(np.min(param), '.2f')}",
        f"Median {param_name} {format(np.median(param), '.2f')}",
        f"High {param_name} {format(np.max(param), '.2f')}",
    ]

    spectra_models = [
        fg.get_model(np.argmin(param), regenerate=True),
        fg.get_model(argmedian(param), regenerate=True),
        fg.get_model(np.argmax(param), regenerate=True),
    ]

    for fm, label in zip(spectra_models, labels_spectra):
        fm.print_results()
        fm.plot()
        plt.title(label)
        plt.show()
        print(label)


def label_components_ica(eeg_data, ica):
    """Assign the IC labels"""

    ic_labels = label_components(eeg_data, ica, method="iclabel")
    labels = ic_labels["labels"]
    return labels, ic_labels


def get_indices_values(labels, ic_labels, threshold=0.9):
    """Get the indices of the labels"""
    element_indices = {}
    element_indices["bad_prob_class"] = []
    probabilities = ic_labels["y_pred_proba"]

    for i, element in enumerate(labels):
        if probabilities[i] > threshold:
            if element not in element_indices:
                element_indices[element] = []
            element_indices[element].append(i)
        else:
            element_indices["bad_prob_class"].append(i)

    # display(element_indices)
    return element_indices


def plot_removed_components(
    ica,
    eeg_data,
    figure_path,
    subject,
    labeled_components,
    chosen_components: list | None = None,
    save_fig=False,
):
    """Plot the components labeled as eye blink, heart beat, line noise, muscle artifact, channel noise"""
    exclude = []
    if chosen_components is None:
        chosen_components = [
            "eye blink",
            "heart beat",
            "line noise",
            "muscle artifact",
            "channel noise",
        ]
    for label, indices in labeled_components.items():
        if label in chosen_components:
            ica_plot = ica.plot_sources(eeg_data, show_scrollbars=False, picks=indices)
            print(label, indices)
            figs = ica.plot_properties(
                eeg_data, picks=indices, psd_args={"fmax": 100.0}
            )
            if save_fig:
                for fig, ind in zip(figs, indices):
                    # fig = ica.plot_properties(eeg_data, picks=ind)
                    fig.savefig(
                        f"{figure_path}/sub-{subject}_ICA{str(ind).zfill(3)}_{
                            label.replace(" ", "-")}.png",
                        dpi=300,
                        bbox_inches="tight",
                    )
                exclude = exclude + indices
    return exclude

</document_content>
</document>
<document index="10">
<source>./src/spectral/spectral/preproc.py</source>
<document_content>
""" Function for preprocessing EEG data """
import mne
import matplotlib.pyplot as plt


from eeglabio.utils import export_mne_raw

from meegkit.detrend import detrend
from meegkit import dss
from pyprep.find_noisy_channels import NoisyChannels

from autoreject import Ransac  # noqa
from typing import  Optional, Union, List
from pathlib import Path
import numpy as np
from mne import pick_types

from .epochs import create_epochs
from .utils import load_config, ProjectPaths



# This part could be replaced by read MFF files (set files are just downsampled MFF files saved to the local laptop drive)
def load_data(subject_id: Union[str, int], 
              data_path: Union[str, Path],
              session: str = "01",
              task: str = "rest") -> mne.io.Raw:
    """
    Load EEG data with explicit path and preprocessing parameters.
    
    This version makes all key parameters visible at the function call,
    which can be helpful for debugging and when you need to override
    default settings for specific subjects.
    
    Args:
        subject_id: Subject identifier (will be zero-padded to 3 digits)
        data_path: Path to the folder containing the BIDS-formatted data of the subject
        session: Session identifier (default "01")
        task: Task name (default "rest")
        
    Returns:
        mne.io.Raw: Loaded and preprocessed raw data
        
    Example:
        >>> raw = load_data(
        ...     subject_id=001,
        ...     data_path="/data/eeg_study/raw_bids/sub-001",
        ... )
    """
    # Convert inputs to proper types
    data_path = Path(data_path)
    
    # Format subject ID consistently
    if isinstance(subject_id, int):
        subject_str = f"{subject_id:03d}"
    else:
        subject_str = str(subject_id).zfill(3)
    
    # Build the file path following BIDS structure
    filename = f"sub-{subject_str}_ses-{session}_task-{task}_eeg.set"
    data_file = data_path  / f"ses-{session}" / "eeg" / filename
    
    # Check if file exists with helpful error message
    if not data_file.exists():
        raise FileNotFoundError(
            f"Data file not found: {data_file}\n"
            f"Project path: {data_path}\n"
            f"Looking for: {filename}\n"
            f"Please check that the subject data has been copied to the BIDS directory."
        )
    print(f"Loading data from: {data_file}")
    
    # Load the raw data
    raw = mne.io.read_raw_eeglab(data_file, preload=True)
    print(f"Loaded {len(raw.ch_names)} channels, {raw.times[-1]:.1f} seconds of data")
    
    # Handle ECG channels
    ecg_channels = [ch for ch in raw.ch_names if "ECG" in ch.upper()]
    if ecg_channels:
        ecg_mapping = {ch: 'ecg' for ch in ecg_channels}
        raw.set_channel_types(ecg_mapping)
        print(f"Identified ECG channels: {ecg_channels}")
    
    # Set the channel type for 'VREF' to 'misc' before applying montage
        # Remove 'VREF' channel if it exists
    if 'VREF' in raw.ch_names:
        raw.drop_channels(['VREF'])
        print("Removed 'VREF' channel.")
    # Apply montage
    print("Applying GSN-HydroCel-256 montage...")
    montage = mne.channels.make_standard_montage("GSN-HydroCel-256")
    raw.set_montage(montage, on_missing='warn')
    return raw


def detrending(raw, order=1):
    """Detrend the data"""
    data = raw.get_data().T  # Convert mne data to numpy darray
    data, _, _ = detrend(data, order=order)
    detrended_raw = mne.io.RawArray(
        data.T, raw.info
    )
    detrended_raw.set_annotations(raw.annotations)
    return detrended_raw


def apply_projection(raw):
    """Apply the projection"""
    raw_clean = raw.copy()
    raw_clean.interpolate_bads()
    raw_clean.set_eeg_reference(
        "average", projection=True)  # compute the reference
    raw_ref = raw.copy().add_proj(raw_clean.info["projs"][0])
    return raw_ref.apply_proj()  # apply the reference


def get_bad_lof(raw):
    bad_channels = mne.preprocessing.find_bad_channels_lof(raw)
    # print(bad_channels)
    raw_marked_bad = raw.copy()
    raw_marked_bad.info["bads"].extend(bad_channels)  # add a list of channels
    bad_channels2 = mne.preprocessing.find_bad_channels_lof(raw_marked_bad)
    return bad_channels + bad_channels2

def zapline_clean(raw, fline, ntimes=1, method="line", iter_param=None):
    """Apply the zapline cleaning"""
    raw
    data = raw.get_data().T  # Convert mne data to numpy darray
    sfreq = raw.info["sfreq"]  # Extract the sampling freq
    # Apply MEEGkit toolbox function
    for _ in range(ntimes):
        if method == "iter":
            data, _ = dss.dss_line_iter(data, fline, sfreq, **iter_param)
        elif method == "line":
            data, _ = dss.dss_line(data, fline, sfreq, nremove=15)

    cleaned_raw = mne.io.RawArray(
        data.T, raw.info
    )  # Convert output to mne RawArray again
    cleaned_raw.set_annotations(raw.annotations)
    return cleaned_raw


def apply_pyprep(raw: mne.io.Raw, output: str = "all", as_dict=True) -> List[str]:
    """Apply the pyprep cleaning"""
    temp = raw.copy().resample(125)
    nd = NoisyChannels(temp, random_state=1337)
    #nd.find_bad_by_correlation(
    #    correlation_secs=1.0, correlation_threshold=0.4, frac_bad=0.01
    # )
    #nd.find_bad_by_deviation(deviation_threshold=5.0)
    if output == "all":
        nd.find_all_bads(ransac=True, channel_wise=True, max_chunk_size=None)
        print("bad all", nd.get_bads(verbose=True))
        return nd.get_bads(verbose=True, as_dict=as_dict)
    else:
        nd.find_bad_by_correlation(
         correlation_secs=1.0, correlation_threshold=0.4, frac_bad=0.01
        )
        nd.find_bad_by_deviation(deviation_threshold=5.0)
        nd.find_bad_by_ransac(n_samples=50, sample_prop=0.25, corr_thresh=0.75,
                              frac_bad=0.4, corr_window_secs=5.0,
                              channel_wise=True, max_chunk_size=None)
        return nd.get_bads(verbose=False, as_dict=as_dict)



def get_bad_channels(raw, save_figs=False):
    """Reject bad channels by RANSAC on the epochs"""
    clean_raw_downsampled = raw.copy().resample(125, npad="auto")
    epochs = create_epochs(clean_raw_downsampled)
    # plot_epochs(epochs, stage="epochs", n_epochs=10, n_channels=25)
    ransac = Ransac(verbose=False, n_jobs=-1)
    _ = ransac.fit_transform(epochs)
    print("\n".join(ransac.bad_chs_))
    #sensor_plot = raw.plot_sensors(show_names=True)
    #if save_figs:
    #    sensor_plot.savefig(
    #        f"{figure_path}/sub-{subject}_bad_sensors.png", dpi=300, bbox_inches="tight"
    #    )
    return [x for x in ransac.bad_chs_]

def compute_ptp_matrix(data, epoch_duration, sfreq):
    """
    Compute peak-to-peak amplitudes for 1-second epochs across all channels.

    Args:
        data (np.ndarray): EEG data with shape (n_channels, n_times)
        epoch_duration (float): Duration of each epoch in seconds
        sfreq (float): Sampling frequency of the data

    Returns:
        np.ndarray: Matrix of peak-to-peak amplitudes with shape (n_epochs, n_channels)
    """
    n_channels, n_times = data.shape
    n_samples_per_epoch = int(epoch_duration * sfreq)
    n_epochs = n_times // n_samples_per_epoch

    # Reshape data into 3D array (n_epochs, n_channels, n_samples_per_epoch)
    epoched_data = np.reshape(
        data[:, : n_epochs * n_samples_per_epoch],
        (n_epochs, n_channels, n_samples_per_epoch),
    )

    # Compute peak-to-peak amplitudes across time for each epoch and channel
    ptp_matrix = np.ptp(epoched_data, axis=2)

    return ptp_matrix


def get_bad_annotations(
    raw_filtered, peak_threshold=2.5, epoch_duration=1.0, extend_bad_duration=1.5
):
    """get bad annotations with high peak-to-peak amplitude"""
    picks = pick_types(raw_filtered.info, meg=False, eeg=True, exclude="bads")

    # Assume 'raw' is your mne.io.Raw object
    data = raw_filtered.get_data(picks=picks)
    sfreq = raw_filtered.info["sfreq"]

    # Compute peak-to-peak amplitude matrix
    ptp_matrix = compute_ptp_matrix(data, epoch_duration, sfreq)
    # print(ptp_matrix)
    peak = peak_threshold * np.median(ptp_matrix)

    print(peak)

    annotations, bads = mne.preprocessing.annotate_amplitude(
        raw_filtered,
        peak=peak,
        flat=None,
        bad_percent=5,
        min_duration=0.005,
        picks=picks,
        verbose=True,
    )

    sfreq = raw_filtered.info["sfreq"]  # Sampling frequency
    # Number of samples to extend
    extend_samples = int(extend_bad_duration * sfreq)

    updated_annotations = []
    for ann in annotations:
        onset = ann["onset"]
        duration = ann["duration"]
        description = ann["description"]

        new_onset = max(0, onset - extend_samples / sfreq)
        new_duration = duration + 2 * extend_samples / sfreq
        updated_annotations.append(
            {
                "onset": new_onset,
                "duration": new_duration,
                "description": description,
                "orig_time": None,
            }
        )

    onset = [ann["onset"] for ann in updated_annotations]
    duration = [ann["duration"] for ann in updated_annotations]
    description = [ann["description"] for ann in updated_annotations]
    return mne.Annotations(onset, duration, description, orig_time=None)

def reject_log_to_annotations(reject_log, epochs):
    """
    Convert a reject_log from autoreject into MNE annotations.

    This function identifies the time spans of bad epochs and creates
    MNE annotations for them, which can be added back to the raw object.

    Args:
        reject_log: The RejectLog instance from autoreject.
        epochs: The MNE epochs object from which the reject_log was generated.

    Returns:
        mne.Annotations: Annotations object marking the bad segments.
    """
    bad_epoch_indices = np.where(reject_log.bad_epochs)[0]

    if len(bad_epoch_indices) == 0:
        return mne.Annotations(onset=[], duration=[], description=[])

    # Get the event timings from the epochs object
    onsets = epochs.events[bad_epoch_indices, 0] / epochs.info["sfreq"]
    duration = len(epochs.times) / epochs.info["sfreq"]

    # Create annotations for each bad epoch
    bad_annotations = mne.Annotations(
        onset=onsets,
        duration=[duration] * len(onsets),
        description=["bad_autoreject"] * len(onsets),
        orig_time=epochs.info.get('meas_date')
    )

    return bad_annotations
</document_content>
</document>
<document index="11">
<source>./src/spectral/spectral/specparam.py</source>
<document_content>
import pandas as pd
import numpy as np
from specparam.core.funcs import infer_ap_func
from specparam.core.info import get_ap_indices


def specparam2pandas(fg):
    """
    Converts a SpectralGroupModel object into a pandas DataFrame, with peak parameters and
    corresponding aperiodic fit information.

    Args:
    -----
    fg : specpramGroup
        The SpectralGroupModel object containing the fitting results.

    Returns:
    --------
    peaks_df : pandas.DataFrame
        A DataFrame with the peak parameters and corresponding aperiodic fit information.
        The columns are:
        - 'CF': center frequency of each peak
        - 'PW': power of each peak
        - 'BW': bandwidth of each peak
        - 'error': fitting error of the aperiodic component
        - 'r_squared': R-squared value of the aperiodic fit
        - 'exponent': exponent of the aperiodic component
        - 'offset': offset of the aperiodic component
        - 'knee': knee parameter of the aperiodic component [if is initially present in the fg object]
    Notes:
    ------
    This function creates two DataFrames. The first DataFrame `specparam_aperiodic`
    contains the aperiodic fit information and is based on the `aperiodic_params`
    attribute of the SpectralGroupModel object. The columns are inferred using the
    `get_ap_indices()` and `infer_ap_func()` functions from the specparam package.
    The second DataFrame `peak_df` contains the peak parameters and is based on the
    `peak_params` attribute of the SpectralGroupModel object. The column names are renamed
    to match the headers of `fooof_aperiodic`, and the 'ID' column is cast to integer.
    The two DataFrames are then merged based on a shared 'ID' column.
    """

    specparam_aperiodic = (
        pd.DataFrame(
            fg.get_params("aperiodic_params"),
            columns=get_ap_indices(
                infer_ap_func(np.transpose(fg.get_params("aperiodic_params")))
            ),
        )
        .assign(error=fg.get_params("error"), r_squared=fg.get_params("r_squared"))
        .reset_index(names=["ID"])
    )
    return (
        pd.DataFrame(fg.get_params("peak_params"))  # prepare peaks dataframe
        .set_axis(["CF", "PW", "BW", "ID"], axis=1)  # rename cols
        .astype({"ID": int})
        .join(specparam_aperiodic.set_index("ID"), on="ID")
    )
</document_content>
</document>
<document index="12">
<source>./src/spectral/spectral/utils.py</source>
<document_content>
"""Utils for helping the analysis of the data"""

import os
from datetime import datetime
from pathlib import Path
import toml  # Use toml instead of ConfigParser
from typing import Dict, Optional, Union
import mne

def print_timestamp(prefix: str = ""):
    """
    Print the current timestamp with an optional prefix message.
    
    Args:
        prefix: Optional message to print before the timestamp
    """
    now = datetime.now()
    timestamp = now.strftime("%Y-%m-%d %H:%M:%S")
    
    if prefix:
        print(f"{prefix}: {timestamp}")
    else:
        print(timestamp)


def find_project_root(start_path: Optional[Path] = None, 
                     marker_file: str = 'settings.toml') -> Path:
    """
    Find the project root by looking for a marker file (like settings.toml).
    
    This function starts from the current directory (or a specified path) and
    walks up the directory tree until it finds the marker file or reaches the
    filesystem root.
    
    Args:
        start_path: Starting directory (defaults to current working directory)
        marker_file: Name of file that marks the project root
        
    Returns:
        Path to the project root directory
        
    Raises:
        FileNotFoundError: If marker file cannot be found
    """
    if start_path is None:
        start_path = Path.cwd()
    else:
        start_path = Path(start_path).resolve()
    
    current = start_path
    
    # Walk up the directory tree
    while current != current.parent:  # Stop at filesystem root
        if (current / marker_file).exists():
            return current
        current = current.parent
    
    # If we get here, we didn't find the marker file
    raise FileNotFoundError(
        f"Could not find '{marker_file}' in any parent directory of {start_path}"
    )



def load_config(config_path: Optional[Path] = None) -> Dict:
    """
    Load configuration from a TOML file.
    
    This function loads all your analysis parameters from a single source,
    making your analysis reproducible and easy to modify.
    """
    if config_path is None:
        project_root = find_project_root()
        config_path = project_root / 'settings.toml'
    else:
        config_path = Path(config_path)
    
    if not config_path.exists():
        raise FileNotFoundError(f"Configuration file not found: {config_path}")
    
    # Load and return the TOML configuration
    with open(config_path, 'r') as f:
        config = toml.load(f)
    
    # You can add validation here if needed
    _validate_config(config)
    
    return config

def _validate_config(config: Dict):
    """
    Validate that the configuration has all required fields.
    
    This helps catch configuration errors early, before they cause
    mysterious failures deep in your analysis pipeline.
    """
    required_sections = ['paths', 'preprocessing']
    for section in required_sections:
        if section not in config:
            raise ValueError(f"Missing required section in config: [{section}]")
    
    # Check that channels_to_remove is a list
    if 'channels_to_remove' in config['preprocessing']:
        if not isinstance(config['preprocessing']['channels_to_remove'], list):
            raise ValueError("channels_to_remove must be a list")
 

class ProjectPaths:
    """
    Simple path management for EEG analysis projects.
    
    Organizes files into just three main categories:
    - data: your raw files
    - processing: intermediate files while you work
    - outputs: final results to share
    """
    
    def __init__(self, subject_id: Union[str, int], config_path: Optional[Path] = None):
        """Initialize paths for a specific subject."""
        # Format subject ID with leading zeros (1 becomes "001")
        self.subject_id = f"{int(subject_id):03d}"
        self.subject = f"sub-{self.subject_id}"
        
        # Get project root from config
        self.config = load_config(config_path)
        self.root = Path(self.config['paths']['project_root']).resolve()
        
        # Set up the simple structure
        self._setup_paths()
    
    def _setup_paths(self):
        """Create a simple, logical structure for your project."""
        
        # Raw data - keep it safe and separate
        self.data = self.root / 'data' / 'raw'  / self.subject
        # Processing - all intermediate files go here
        self.processing = self.root /'data' / 'derrivatives' / self.subject
        self.preprocessed = self.processing / 'preprocessed'  # Cleaned/filtered data
        self.epochs = self.processing / 'epochs'              # Epoched data
        self.analysis = self.processing / 'analysis'          # PSD, connectivity, etc.
        
        # Outputs - things you want to keep and share
        self.outputs = self.root / 'outputs' / self.subject
        self.figures = self.outputs / 'figures'
        self.reports = self.outputs / 'reports'
        
        # One place for all the miscellaneous stuff
        self.logs = self.processing / 'logs'
    
    def make_filename(self, description: str, extension: str = '.fif') -> str:
        """
        Create consistent filenames.
        Example: make_filename('filtered-1-45Hz') → 'sub-001_filtered-1-45Hz.fif'
        """
        return f"{self.subject}_{description}{extension}"
    
    def create_directories(self):
        """Create all directories for this subject."""
        all_dirs = [
            self.data,
            self.preprocessed,
            self.epochs, 
            self.analysis,
            self.figures,
            self.reports,
            self.logs
        ]
        
        for directory in all_dirs:
            directory.mkdir(parents=True, exist_ok=True)
        
        print(f"Created directories for {self.subject}")
        print(f"Project root: {self.root}")
    
    def clean_empty_directories(self):
        """
        Remove empty directories in the derivatives folder.
        
        Useful for keeping your project tidy by removing folders
        from analyses you didn't run.
        """
        for root, dirs, files in os.walk(self.subject_derivatives, topdown=False):
            for dir_name in dirs:
                dir_path = Path(root) / dir_name
                if not any(dir_path.iterdir()):
                    dir_path.rmdir()
                    print(f"✗ Removed empty: {dir_path.relative_to(self.project_root)}")
    
    def show(self, selection: Optional[list[str]] = None):
        """
        Displays specified or all project paths in a formatted way.

        Args:
            selection (Optional[list[str]]): A list of path attribute names to display.
                                             If None, all paths will be shown.
                                             Example: ['data', 'figures', 'epochs']
        """
        print("─" * 60)
        print(f"Paths for {self.subject}")
        print(f"Project Root: {self.root}")
        print("─" * 60)

        # 1. Discover all attributes that are Path objects
        all_path_attrs = {
            key: val
            for key, val in self.__dict__.items()
            if isinstance(val, Path) and key != 'root'
        }

        # 2. Determine which paths to display based on user selection
        paths_to_display = {}
        if selection is None:
            # If no selection, use all discovered paths
            paths_to_display = all_path_attrs
        else:
            # If user made a selection, filter for those
            for name in selection:
                if name in all_path_attrs:
                    paths_to_display[name] = all_path_attrs[name]
                else:
                    print(f"  ✗ Warning: '{name}' is not a valid path attribute.")
        
        if not paths_to_display:
            return # Exit if nothing to show

        # 3. Format and print the output
        max_key_length = max(len(key) for key in paths_to_display.keys()) + 1
        
        for key, path in sorted(paths_to_display.items()):
            print(f"  {key:<{max_key_length}}: {path}")
            
        print("─" * 60)
</document_content>
</document>
<document index="13">
<source>./src/spectral/spectral/viz.py</source>
<document_content>
""" Functions to plot data and PSD """
from pathlib import Path
import matplotlib.pyplot as plt
import mne
import numpy as np


def plot_step(temp, subject,  figures_path, stage="raw", duration=50.0, n_channels=50, fmax=100.0):
    """Plot raw data and PSD of the data"""
    raw_plot = mne.viz.plot_raw(
        temp.copy(),
        duration=duration,
        scalings=dict(eeg=1e-4),
        n_channels=n_channels,
        show_scrollbars=False,
        title=f"sub-{subject}_{stage}",
    )
    Path(figures_path).mkdir(parents=True, exist_ok=True)
    raw_plot.savefig(
        f"{figures_path}/sub-{subject}_{stage}.png", dpi=300, bbox_inches="tight"
    )
    plt.close()

    # Plot is saved to monitor the quality of the data
    fig, ax = plt.subplots(figsize=(10, 5))
    raw_psd = temp.compute_psd(
        fmax=fmax, method="welch", picks="eeg", exclude="bads"
    ).plot(average=False, picks="eeg", exclude="bads", show=False,
           axes=ax, amplitude=False)
    ax.set_title(f"sub-{subject} {stage} PSD")
    raw_psd.savefig(
        f"{figures_path}/sub-{subject}_{stage}-psd.png", dpi=300, bbox_inches="tight"
    )
    fig.show()


def plot_epochs(epochs, figures_path, subject, stage="epochs", n_epochs=10, n_channels=10, fmax=100.0):
    """Plot the  epoch data"""
    epochs_plot = epochs.copy().average().detrend().plot_joint()
    Path(figures_path).mkdir(parents=True, exist_ok=True)
    epochs_plot.savefig(
        f"{figures_path}/sub-{subject}_{stage}.png", dpi=300, bbox_inches="tight"
    )
    epochs_timeseries = mne.viz.plot_epochs(
        epochs=epochs,
        picks="eeg",
        show=False,
        n_epochs=n_epochs,
        n_channels=n_channels,
        scalings=dict(eeg=1e-4),
        show_scrollbars=False,
    )
    epochs_plot_psd = epochs.compute_psd(
        method="welch", picks="eeg", fmax=fmax, exclude="bads"
    ).plot(average=False, picks="eeg", exclude="bads", show=False, amplitude=False)
    epochs_plot_psd.savefig(
        f"{figures_path}/sub-{subject}_{stage}_psd.png", dpi=300, bbox_inches="tight"
    )
    return (epochs_plot, epochs_timeseries, epochs_plot_psd)


def plot_bad_channels(raw, subject, figures_path):
    """Plot the sensor locations"""
    Path(figures_path).mkdir(parents=True, exist_ok=True)
    bad_channels = raw.copy().pick(raw.info["bads"])
    bad_channel_plot = bad_channels.plot(
        duration=300.0,
        scalings=dict(eeg=1e-4),
        show_scrollbars=False,
    )
    # Path(figure_path).mkdir(parents=True, exist_ok=True)
    bad_channel_plot.savefig(
        f"{figures_path}/sub-{subject}_bad_channel_plot.png",
        dpi=300,
        bbox_inches="tight",)
    plt.close()
    sensor_plot = raw.plot_sensors(show_names=True)
    sensor_plot.savefig(
        f"{figures_path}/sub-{subject}_sensors.png", dpi=300, bbox_inches="tight"
    )
    plt.close()


def visualise_bad_epochs(reject_log):
    """Visualise the bad epochs and channels."""
    bads = np.logical_or(reject_log.labels == 1, reject_log.labels == 2)
    plt.imshow(bads, cmap="viridis")
    plt.colorbar(orientation="horizontal", pad=0.1)
    plt.show()

    print(
        f"Currently removed number of epochs {
            np.sum(reject_log.bad_epochs)}"
    )
    # print(bads)
    # print(bads.shape)
    good_epochs_percentage = (1 - bads.mean(axis=1)) * 100

    # print("Percentage of bad epochs in each epoch:")
    # display(good_epochs_percentage)

    print("Percentage of good epochs in each  candidate for removal epoch:")
    for i in range(0, len(good_epochs_percentage)):
        if good_epochs_percentage[i] < 75:
            print(f"Epoch {i}: {good_epochs_percentage[i]:.2f}%")
            # print(f"Epoch {i}: {good_epochs_percentage[i]:.2f}%")
</document_content>
</document>
</documents>
